{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9d6785-4288-47a6-94e5-412a5a4909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f262c5d1-1748-4832-bd01-263e6d6d4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"/home/neeraj/Documents/face_detx/runs-20241009T055735Z-001/runs/detect/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e28485-ed01-4e4d-bba8-8434d68aee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45565c5-5f7a-4c9f-9af3-16b105b94b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Could not open webcam.\")\n",
    "#     exit()\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"Error: Failed to capture frame.\")\n",
    "#         break\n",
    "\n",
    "#     results = model.predict(source=frame, conf=0.25)\n",
    "#     annotated_frame = results[0].plot()\n",
    "#     print(results[0])\n",
    "\n",
    "#     cv2.imshow('YOLOv8 Face Detex', annotated_frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b325a4ef-3e67-4642-95c5-1e2d06743118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Read an image (replace 'path_to_image' with the path to your image)\n",
    "# image_path = '/home/neeraj/Pictures/Webcam/2024-01-06-111643.jpg'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# if image is None:\n",
    "#     print(\"Error: Could not load image.\")\n",
    "#     exit()\n",
    "\n",
    "# # Predict objects (e.g., faces) in the image\n",
    "# results = model.predict(source=image, conf=0.25)\n",
    "\n",
    "# # Access the first prediction result (since we're processing one image)\n",
    "# print(results[0])  # This will display the detection result details\n",
    "\n",
    "# # Annotate the image with the predictions (draw bounding boxes, labels, etc.)\n",
    "# annotated_image = results[0].plot()\n",
    "\n",
    "# # Display the annotated image with OpenCV\n",
    "# cv2.imshow('YOLOv8 Face Detection', annotated_image)\n",
    "\n",
    "# # Wait for a key press and close the window\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99032332-0d19-4003-9ce2-127cc3d37b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from ultralytics import YOLO  # Ensure YOLOv8 is imported properly\n",
    "\n",
    "# # Load the YOLOv8 model (replace 'yolov8n.pt' with your actual model)\n",
    "# model = YOLO(\"/home/neeraj/Documents/face_detx/runs-20241009T055735Z-001/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# # Read an image (replace 'path_to_image' with the path to your image)\n",
    "# image_path = '/home/neeraj/Pictures/Webcam/2024-01-06-111643.jpg'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# if image is None:\n",
    "#     print(\"Error: Could not load image.\")\n",
    "#     exit()\n",
    "\n",
    "# # Predict objects (e.g., faces) in the image\n",
    "# results = model.predict(source=image, conf=0.25)\n",
    "\n",
    "# # Access the first prediction result (since we're processing one image)\n",
    "# result = results[0]\n",
    "\n",
    "# # Loop through the detections to get the bounding box coordinates\n",
    "# for box in result.boxes:\n",
    "#     # Extract bounding box coordinates (x1, y1, x2, y2)\n",
    "#     x1, y1, x2, y2 = box.xyxy[0]  # xyxy format: top-left and bottom-right corners\n",
    "#     conf = box.conf[0]            # Confidence score of the detection\n",
    "    \n",
    "#     # Print the bounding box coordinates and confidence score\n",
    "#     print(f\"Bounding Box: x1={x1}, y1={y1}, x2={x2}, y2={y2}, Confidence: {conf}\")\n",
    "\n",
    "# # Annotate the image with the predictions (draw bounding boxes, labels, etc.)\n",
    "# annotated_image = result.plot()\n",
    "\n",
    "# # Display the annotated image with OpenCV\n",
    "# cv2.imshow('YOLOv8 Face Detection', annotated_image)\n",
    "\n",
    "# # Wait for a key press and close the window\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009eb64e-81d8-4064-b1e5-00feeb17b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, y1, x2, y2 = round(x1.item()), round(y1.item()), round(x2.item()), round(y2.item())\n",
    "# h, w, _ = image.shape\n",
    "# x1, y1 = max(0, x1), max(0, y1)\n",
    "# x2, y2 = min(w, x2), min(h, y2)\n",
    "# face = image[y1:y2, x1:x2]  # Crop the face using YOLOv8's bounding box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11a8c7a-ee0b-4860-92b1-923e7e5d67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face = image[x1:x2, y1:y2]\n",
    "# if(face.size > 0):  # Check if face is not empty\n",
    "#     face = cv2.resize(face, (160, 160))\n",
    "#     cv2.imshow('Cropped Face', face)\n",
    "# else:\n",
    "#     print(f\"Warning: Invalid face crop for bounding box x1={x1}, y1={y1}, x2={x2}, y2={y2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad5c838-f0e5-4e8a-af34-10ab63dc0e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 292.6ms\n",
      "Speed: 16.2ms preprocess, 292.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Bounding Box 1: x1=574, y1=272, x2=747, y2=463\n",
      "Face 1 saved to cropped_face_1.jpg\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# from ultralytics import YOLO \n",
    "# import os\n",
    "\n",
    "# model = YOLO(\"/home/neeraj/Documents/face_detx/runs-20241009T055735Z-001/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# image_path = '/home/neeraj/Pictures/Webcam/2024-01-06-111643.jpg'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# if image is None:\n",
    "#     print(\"Error: Could not load image.\")\n",
    "#     exit()\n",
    "\n",
    "# results = model.predict(source=image, conf=0.25)\n",
    "\n",
    "# result = results[0]\n",
    "# cv2.imshow(\"Face_deteX\", result.plot())\n",
    "# for i, box in enumerate(result.boxes):\n",
    "#     x1, y1, x2, y2 = box.xyxy[0]\n",
    "#     x1, y1, x2, y2 = round(x1.item()), round(y1.item()), round(x2.item()), round(y2.item())\n",
    "\n",
    "#     print(f\"Detected Bounding Box {i+1}: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "#     h, w, _ = image.shape\n",
    "#     x1, y1 = max(0, x1), max(0, y1)\n",
    "#     x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "#     face = image[y1:y2, x1:x2]\n",
    "    \n",
    "#     if face.size > 0:  \n",
    "#         face = cv2.resize(face, (160, 160))\n",
    "#         cv2.imshow(f'Face {i+1}', face)\n",
    "#         save_path = f\"cropped_face_{i+1}.jpg\"\n",
    "#         cv2.imwrite(save_path, face)\n",
    "#         print(f\"Face {i+1} saved to {save_path}\")\n",
    "#     else:\n",
    "#         print(f\"Warning: Invalid face crop for bounding box x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38be9ca0-94e1-4239-ab4b-8d828a994876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocess\n",
    "# mean, std = face.mean(),face.std()\n",
    "# face = (face-mean)/std\n",
    "# face = np.expand_dims(face, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e1cb1a-7d3f-482a-a927-622a61950cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_facenet import FaceNet\n",
    "# embedder = FaceNet()\n",
    "\n",
    "# def get_embeddings(face_img):\n",
    "#     # face_img = face_img.astype('float32')\n",
    "#     # face_img = np.expand_dims(face_img,axis=0)\n",
    "#     y_hat = embedder.embeddings(face_img)\n",
    "#     return y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d20580b-5810-47f0-8cb1-ad39aab4c93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "# labels = [\"neeraj\"]\n",
    "# embeddings = []\n",
    "# embed_neeraj = get_embeddings(face)\n",
    "# embeddings.append(embed_neeraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6182f42f-598b-481d-873e-f71b8d4a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = []\n",
    "embeddings_list = []\n",
    "lables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91ebca56-9dc0-46d1-b6c7-ff0991545df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  Neeraj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 277.3ms\n",
      "Speed: 5.3ms preprocess, 277.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 341.1ms\n",
      "Speed: 5.3ms preprocess, 341.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\n",
      "0: 480x640 (no detections), 166.3ms\n",
      "Speed: 3.9ms preprocess, 166.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 154.1ms\n",
      "Speed: 5.9ms preprocess, 154.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 154.8ms\n",
      "Speed: 5.4ms preprocess, 154.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 157.4ms\n",
      "Speed: 3.9ms preprocess, 157.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 152.4ms\n",
      "Speed: 3.8ms preprocess, 152.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\n",
      "0: 480x640 1 person, 162.4ms\n",
      "Speed: 3.6ms preprocess, 162.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\n",
      "0: 480x640 1 person, 155.9ms\n",
      "Speed: 4.4ms preprocess, 155.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\n",
      "0: 480x640 1 person, 163.9ms\n",
      "Speed: 2.7ms preprocess, 163.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 162.0ms\n",
      "Speed: 4.3ms preprocess, 162.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "0: 480x640 1 person, 163.1ms\n",
      "Speed: 6.5ms preprocess, 163.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "0: 480x640 1 person, 163.8ms\n",
      "Speed: 4.8ms preprocess, 163.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 158.8ms\n",
      "Speed: 6.4ms preprocess, 158.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 153.6ms\n",
      "Speed: 7.8ms preprocess, 153.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 170.5ms\n",
      "Speed: 2.1ms preprocess, 170.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "0: 480x640 1 person, 213.4ms\n",
      "Speed: 3.2ms preprocess, 213.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 168.4ms\n",
      "Speed: 4.8ms preprocess, 168.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 156.7ms\n",
      "Speed: 3.5ms preprocess, 156.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 158.5ms\n",
      "Speed: 5.4ms preprocess, 158.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 166.5ms\n",
      "Speed: 6.8ms preprocess, 166.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 164.1ms\n",
      "Speed: 2.5ms preprocess, 164.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\n",
      "0: 480x640 1 person, 168.5ms\n",
      "Speed: 2.7ms preprocess, 168.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "0: 480x640 1 person, 168.5ms\n",
      "Speed: 4.4ms preprocess, 168.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\n",
      "0: 480x640 1 person, 158.8ms\n",
      "Speed: 7.9ms preprocess, 158.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\n",
      "0: 480x640 1 person, 161.4ms\n",
      "Speed: 2.7ms preprocess, 161.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "0: 480x640 1 person, 165.7ms\n",
      "Speed: 4.7ms preprocess, 165.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 170.3ms\n",
      "Speed: 3.0ms preprocess, 170.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "0: 480x640 1 person, 171.4ms\n",
      "Speed: 4.7ms preprocess, 171.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "0: 480x640 1 person, 163.2ms\n",
      "Speed: 3.7ms preprocess, 163.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "0: 480x640 1 person, 155.4ms\n",
      "Speed: 3.0ms preprocess, 155.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "0: 480x640 1 person, 158.5ms\n",
      "Speed: 2.2ms preprocess, 158.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 160.0ms\n",
      "Speed: 2.4ms preprocess, 160.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "0: 480x640 1 person, 163.3ms\n",
      "Speed: 3.6ms preprocess, 163.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\n",
      "0: 480x640 1 person, 240.0ms\n",
      "Speed: 2.9ms preprocess, 240.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Bounding boxes and embeddings stored successfully.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "model = YOLO(\"/home/neeraj/Documents/face_detx/runs-20241009T055735Z-001/runs/detect/train/weights/best.pt\")\n",
    "embedder = FaceNet()\n",
    "\n",
    "CONF_THRESHOLD = 0.85\n",
    "\n",
    "name = input(\"Enter your name: \")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    results = model(source=frame)\n",
    "    result = results[0]\n",
    "\n",
    "    for i, box in enumerate(result.boxes):\n",
    "        confidence = box.conf[0]\n",
    "\n",
    "        if confidence >= CONF_THRESHOLD:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = round(x1.item()), round(y1.item()), round(x2.item()), round(y2.item())\n",
    "\n",
    "            # print(f\"Detected Bounding Box {i+1}: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "\n",
    "            face_resized = cv2.resize(face, (160, 160))\n",
    "\n",
    "            face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
    "            mean, std = face.mean(),face.std()\n",
    "            face = (face_rgb-mean)/std\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "\n",
    "            embedding = embedder.embeddings(face)[0]\n",
    "\n",
    "            bounding_boxes.append((x1, y1, x2, y2))\n",
    "            embeddings_list.append(embedding)\n",
    "            lables.append(name)\n",
    "\n",
    "            # cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('YOLOv8 FacedetX', result.plot())\n",
    "\n",
    "    # Check if 5 seconds have passed\n",
    "    if len(lables)==30:\n",
    "        break\n",
    "\n",
    "    # Press 'q' to quit manually\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# Save the bounding boxes and embeddings to a compressed file\n",
    "# np.savez_compressed('face_data.npz', bounding_boxes=bounding_boxes, embeddings=embeddings_list)\n",
    "\n",
    "print(\"Bounding boxes and embeddings stored successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46e6c9a2-7da2-43e1-af7e-127f4158a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lables: 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"lables: {len(lables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6efd8640-8e65-4f3e-804b-603039baa4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj', 'Neeraj']\n"
     ]
    }
   ],
   "source": [
    "print(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04ee7c5e-1337-416d-8d9b-d24f25486c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a0fa6-d419-4065-8835-7dc5b384c018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
